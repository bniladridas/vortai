# SPDX-FileCopyrightText: Copyright (c) 2025 Niladri Das <bniladridas>
# SPDX-License-Identifier: MIT

"""
Gemini AI SDK - Simple interface for Google's Gemini AI models.
"""

import os
import uuid
import tempfile
import requests
import logging
from typing import Optional, Dict, Any
import google.generativeai as genai
from gtts import gTTS
from google import genai as google_genai
from google.genai import types
from . import models
from .image_providers import ImageGenerationService

try:
    import redis
except ImportError:
    redis = None


class GeminiAI:
    """SDK for interacting with Gemini AI models."""

    def __init__(self, api_key: Optional[str] = None):
        """Initialize with API key."""
        api_key = api_key or os.environ.get("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY is required")
        genai.configure(api_key=api_key)
        self.cache = None
        redis_url = os.environ.get("REDIS_URL")
        if redis and redis_url:
            try:
                self.cache = redis.from_url(redis_url)
            except redis.exceptions.RedisError as e:
                logging.warning(f"Could not connect to Redis: {e}. Falling back to in-memory cache.")
        if self.cache is None:
            self.cache = {}  # In-memory cache
        self.image_service = ImageGenerationService()

    def generate_text(self, prompt: str) -> str:
        """Generate text response from prompt."""
        if not prompt or len(prompt) > 5000:
            raise ValueError("Invalid prompt")
        cache_key = str(hash(prompt))
        if isinstance(self.cache, dict):
            if cache_key in self.cache:
                return self.cache[cache_key]
        else:
            cached = self.cache.get(cache_key)
            if cached:
                return cached.decode("utf-8") if isinstance(cached, bytes) else cached
        try:
            model = genai.GenerativeModel(models.TEXT_MODEL)
            response = model.generate_content(prompt)
            result = response.text
        except Exception as e:
            raise ValueError(f"Failed to generate text: {e}") from e
        if isinstance(self.cache, dict):
            self.cache[cache_key] = result
        else:
            self.cache.set(cache_key, result)
        return result

    def generate_text_with_thinking(self, prompt: str) -> Dict[str, Any]:
        """Generate text with thinking summary."""
        if not prompt or len(prompt) > 5000:
            raise ValueError("Invalid prompt")
        try:
            client = google_genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))
            response = client.models.generate_content(
                model=models.THINKING_MODEL,
                contents=prompt,
                config={"thinking_config": {"include_thoughts": True}},
            )
        except Exception as e:
            raise ValueError(f"Failed to generate text with thinking: {e}") from e
        main_response = response.text if hasattr(response, "text") else ""
        thinking_summary = []
        if hasattr(response, "candidates") and response.candidates:
            candidate = response.candidates[0]
            if hasattr(candidate, "content") and hasattr(candidate.content, "parts"):
                for part in candidate.content.parts:
                    if hasattr(part, "thought") and part.thought:
                        thinking_summary.append(
                            part.text if hasattr(part, "text") else str(part.thought)
                        )
        return {"response": main_response, "thinking_summary": thinking_summary}

    def generate_text_with_url_context(self, prompt: str) -> str:
        """Generate text with URL context."""
        if not prompt or len(prompt) > 5000:
            raise ValueError("Invalid prompt")
        try:
            client = google_genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))
            url_context_tool = types.Tool(url_context=types.UrlContext())
            response = client.models.generate_content(
                model=models.URL_CONTEXT_MODEL,
                contents=prompt,
                config={"tools": [url_context_tool]},
            )
            return response.text
        except Exception as e:
            raise ValueError(f"Failed to generate text with URL context: {e}") from e

    def text_to_speech(self, text: str) -> str:
        """Convert text to speech and return file path."""
        if not text or len(text) > 1000:
            raise ValueError("Invalid text")
        try:
            filename = f"{uuid.uuid4()}.mp3"
            filepath = os.path.join(tempfile.gettempdir(), "gemini_tts", filename)
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            tts = gTTS(text=text, lang="en", slow=False)
            tts.save(filepath)
            return filepath
        except Exception as e:
            raise ValueError(f"Failed to generate speech: {e}") from e

    def process_text_go(self, text: str) -> str:
        """Process text using Go service for normalization."""
        if not text:
            raise ValueError("No text provided")

        try:
            # Call Go service
            go_service_url = os.environ.get(
                "GO_SERVICE_URL", "http://localhost:8080/process"
            )
            response = requests.post(go_service_url, data={"text": text}, timeout=5)
            response.raise_for_status()
            return response.text.strip()
        except requests.exceptions.RequestException as e:
            # Fallback to Python implementation if Go service is not available
            logging.info(f"Go service unavailable ({e}), using Python fallback")
            return self._process_text_python(text)

    def _process_text_python(self, text: str) -> str:
        """Python fallback for text processing."""
        import re

        # Simple text normalization: trim and normalize spaces
        return re.sub(r"\s+", " ", text.strip())

    def generate_image(self, prompt: str) -> str:
        """Generate image and return file path."""
        if not prompt or len(prompt) > 5000:
            raise ValueError("Invalid prompt")

        model = models.IMAGE_MODEL
        return self.image_service.generate_image(prompt, model)
